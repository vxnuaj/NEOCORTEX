#InformationTheory

Kullback-Leibler (KL) divergence is a means to measure the divergence between two separate probability distributions.

Given probability distributions, $P$ and $Q$, the KL divergence can be computed as:

$D_{KL}(P||Q) = \sum_i P(i) \cdot log(\frac{P(i)}{Q(i)})$