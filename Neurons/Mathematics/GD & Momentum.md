Gradient Descent with Momentum involves computing an exponentially weighted average of our gradients and then use that average to update our weights.

Say we had the surface of the loss function as such:

