Mechanistic interpretability aims to reverse engineering a neural network, to understand how each parameter, each neuron, affects the flow of the architecture to produce it's final output.

It's the neuroscience of artificial intelligence.

A challenge to mechanistic interpretability is overcoming the *[[Curse of Dimensionality]]*, *"how can we understand a function over such a large input space, given exponentially large number of  potential activations, in a compressed period of time?*.

Some solutions are:

- Study smaller sized neural networks
- Focus on the [[Saliency Maps]], around a specific point of interest

And ultimately, examining a neural network as it learns and builds it's complex function isn't as feasible as understanding and studying the parameters themselves. 

Once trained, the parameters are what determine how a model works, through and through.







