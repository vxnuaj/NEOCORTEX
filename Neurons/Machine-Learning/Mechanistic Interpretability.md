Mechanistic interpretability aims to reverse engineering a neural network, to understand how each parameter, each neuron, affects the flow of the architecture to produce it's final output.

It's the neuroscience of artificial intelligence.

A challenge to mechanistic interpretability is overcoming the *[[Curse of Dimensionality]]*, *"how can we understand a function over such a large input space, given exponentially large number of  potential activations, in a compressed period of time?*.

Some solutions are:

- Study smaller sized neural networks
- Focus on the [[Saliency Maps]]









